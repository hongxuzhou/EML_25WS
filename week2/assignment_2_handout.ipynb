{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 - Classification\n",
    "\n",
    "In this assignment, you will gain hands-on coding experience by working on a classification task. **(10 Points)**\n",
    "\n",
    "This notebook will guide you through using logistic regression, LDA, and QDA for classification tasks.\n",
    "\n",
    "**Objectives:**\n",
    "- Apply logistic regression for binary classification.\n",
    "- Understand the differences between LDA and QDA.\n",
    "- Apply LDA and QDA for multivariate data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Plotting Function\n",
    "Make sure you have installed all the necessary packages listed in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import necessary packages\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = mpl.colormaps['coolwarm']\n",
    "\n",
    "def plot_ellipse(mean, cov, color, ax):\n",
    "    v, w = np.linalg.eigh(cov)\n",
    "    u = w[0] / np.linalg.norm(w[0])\n",
    "    angle = np.arctan(u[1] / u[0])\n",
    "    angle = 180 * angle / np.pi  # convert to degrees\n",
    "    # filled Gaussian at 2 standard deviation\n",
    "    ell = mpl.patches.Ellipse(\n",
    "        mean,\n",
    "        2 * v[0] ** 0.5,\n",
    "        2 * v[1] ** 0.5,\n",
    "        angle=180 + angle,\n",
    "        facecolor=color,\n",
    "        edgecolor=\"black\",\n",
    "        linewidth=2,\n",
    "    )\n",
    "    ell.set_clip_box(ax.bbox)\n",
    "    ell.set_alpha(0.4)\n",
    "    ax.add_artist(ell)\n",
    "\n",
    "\n",
    "def plot_result(estimator, X, y, ax):\n",
    "    DecisionBoundaryDisplay.from_estimator(\n",
    "        estimator,\n",
    "        X,\n",
    "        response_method=\"predict_proba\",\n",
    "        plot_method=\"pcolormesh\",\n",
    "        ax=ax,\n",
    "        cmap=cmap,\n",
    "        alpha=0.3,\n",
    "    )\n",
    "    DecisionBoundaryDisplay.from_estimator(\n",
    "        estimator,\n",
    "        X,\n",
    "        response_method=\"predict_proba\",\n",
    "        plot_method=\"contour\",\n",
    "        ax=ax,\n",
    "        alpha=1.0,\n",
    "        levels=[0.5],\n",
    "    )\n",
    "    y_pred = estimator.predict(X)\n",
    "    X_right, y_right = X[y == y_pred], y[y == y_pred]\n",
    "    X_wrong, y_wrong = X[y != y_pred], y[y != y_pred]\n",
    "    ax.scatter(X_right[:, 0], X_right[:, 1], c=y_right, s=20, cmap=cmap, alpha=0.5)\n",
    "    ax.scatter(\n",
    "        X_wrong[:, 0],\n",
    "        X_wrong[:, 1],\n",
    "        c=y_wrong,\n",
    "        s=30,\n",
    "        cmap=cmap,\n",
    "        alpha=0.9,\n",
    "        marker=\"x\",\n",
    "    )\n",
    "    ax.scatter(\n",
    "        estimator.means_[:, 0],\n",
    "        estimator.means_[:, 1],\n",
    "        c=\"yellow\",\n",
    "        s=200,\n",
    "        marker=\"*\",\n",
    "        edgecolor=\"black\",\n",
    "    )\n",
    "\n",
    "    if isinstance(estimator, LinearDiscriminantAnalysis):\n",
    "        covariance = [estimator.covariance_] * 2\n",
    "    else:\n",
    "        covariance = estimator.covariance_\n",
    "    plot_ellipse(estimator.means_[0], covariance[0], cmap(0.0), ax)\n",
    "    plot_ellipse(estimator.means_[1], covariance[1], cmap(1.0), ax)\n",
    "\n",
    "    ax.set_box_aspect(1)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"bottom\"].set_visible(False)\n",
    "    ax.spines[\"left\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.set(xticks=[], yticks=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a synthetic dataset\n",
    "X, y = make_classification(\n",
    "    n_samples = 256, \n",
    "    n_features = 2,\n",
    "    n_redundant = 0,\n",
    "    n_informative = 2, \n",
    "    n_clusters_per_class = 1, \n",
    "    class_sep = 0.6,\n",
    "    random_state = 37\n",
    ")\n",
    "\n",
    "# Visualize the dataset\n",
    "plt.figure(figsize=(8,8))\n",
    "# Plot the points for Class 0\n",
    "plt.scatter(X[y == 0, 0], X[y == 0, 1], color=cmap(0.0), s=50, label='Class 0', alpha=0.6, edgecolors='k')\n",
    "# Plot the points for Class 1\n",
    "plt.scatter(X[y == 1, 0], X[y == 1, 1], color=cmap(1.0), s=50, label='Class 1', alpha=0.6, edgecolors='k')\n",
    "plt.title('Binary Classification Dataset')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend(['Class 0', 'Class 1'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1:** Check the outputs of the logistic regression model on the test set. To this end, plot a histogram of the outputs for each ground-truth class. Additionally, plot the decision threshold as a vertical line in the histograms. Label your plots and briefly describe the axes and their contents. **(2 Points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Fit a linear regression model\n",
    "logistic_regressor = LogisticRegression()\n",
    "logistic_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_prob_all = logistic_regressor.predict_proba(X_test)\n",
    "\n",
    "# Select the probabilities per class\n",
    "y_pred_prob_class_0 = y_pred_prob_all[y_test == 0, 1]\n",
    "y_pred_prob_class_1 = y_pred_prob_all[y_test == 1, 1]\n",
    "\n",
    "# TODO: Plot the histogram of predicted probabilities for each class. Be sure to provide a title for each plot and label the axes.\n",
    "# TODO: For each class, also plot the decision threshold at 0.5 as a vertical (dashed) line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Briefly describe the Plots: **TODO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2**: Convert the predicted probabilities to binary class labels using a threshold of $0.5$. Based on the resulting confusion matrix, compute the Type I and Type II error rates. **(1.5 Points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = y_pred_prob_all[:, 1]\n",
    "# TODO: Convert the predicted probabilities to binary class labels using a threshold of 0.5.\n",
    "y_pred_class = # TODO \n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred_class)\n",
    "print(f'Accuracy of the Linear Regression Classifier: {accuracy:.2f}')\n",
    "\n",
    "# Get the confusion matrix\n",
    "cm_matrix = confusion_matrix(y_test, y_pred_class)\n",
    "\n",
    "# Visualize the confusion matrix with a heatmap\n",
    "cm_matrix_df = pd.DataFrame(cm_matrix, index=['True 0', 'True 1'], columns=['Predicted 0', 'Predicted 1'])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_matrix_df, annot=True, fmt='g', cmap='Blues', cbar=False)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "FP = cm_matrix[0, 1]  # False Positives\n",
    "FN = cm_matrix[1, 0]  # False Negatives\n",
    "TN = cm_matrix[0, 0]  # True Negatives\n",
    "TP = cm_matrix[1, 1]  # True Positives\n",
    "\n",
    "# TODO: Compute the Type I and Type II error rates\n",
    "type1_error_rate =  #TODO\n",
    "type2_error_rate =  #TODO\n",
    "\n",
    "print(f'Type I Error Rate (False Positive Rate): {type1_error_rate:.2f}')\n",
    "print(f'Type II Error Rate (False Negative Rate): {type2_error_rate:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3:** In Task 1, you created 2 plots (one per class), each divided along the x-axis into two decision regions by the decision threshold. Overall, this results in 4 regions. Identify which regions contain the predictions for True Negatives (TN), True Positives (TP), and False Negatives (FN). **(1.5 Points)**\n",
    "\n",
    "- TN: **TODO**\n",
    "- TP: **TODO**\n",
    "- FN: **TODO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4:** Determine (and visualize) the correct predictions. **(1 Point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Assign correctly classified points to `X_right` and their corresponding labels to `y_right`. Then do the same for wrongly classified points.\n",
    "X_right, y_right = # TODO\n",
    "X_wrong, y_wrong = # TODO\n",
    "\n",
    "# Visualize the decision boundaries\n",
    "# Define the mesh grid\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300),\n",
    "                     np.linspace(y_min, y_max, 300))\n",
    "\n",
    "# Plot the decision boundary for logistic regression\n",
    "Z = logistic_regressor.predict(np.c_[xx.ravel(), yy.ravel()]) # Predict using logistic regression on the mesh grid\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')\n",
    "plt.scatter(\n",
    "    X_right[:, 0], \n",
    "    X_right[:, 1], \n",
    "    c=y_right, \n",
    "    cmap='coolwarm', \n",
    "    alpha=0.9, \n",
    "    label='Correctly classified', \n",
    "    edgecolors='k'\n",
    ")\n",
    "plt.scatter(\n",
    "    X_wrong[:, 0],\n",
    "    X_wrong[:, 1],\n",
    "    c=y_wrong,\n",
    "    cmap='coolwarm',\n",
    "    alpha=0.9,\n",
    "    marker=\"x\",\n",
    "    label='Misclassified'\n",
    ")\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], marker='o', linestyle='', label='Correctly classified',\n",
    "           markerfacecolor='gray', markeredgecolor='k', markersize=6),\n",
    "    Line2D([0], [0], marker='x', linestyle='', label='Misclassified', markeredgewidth=2.0,\n",
    "           markeredgecolor='black', markersize=6)\n",
    "]\n",
    "\n",
    "plt.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "plt.title('Binary Classification Dataset - Predictions')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Gaussian-Distributed Data: LDA and QDA\n",
    "\n",
    "Below, we generate three datasets, each drawn from different two-dimensional Gaussian distributions.\n",
    "\n",
    "First, we define a function to generate synthetic data. It creates two blobs centered\n",
    "at $(0, 0)$ and $(1, 1)$. Each blob is assigned a specific class. The dispersion of\n",
    "the blobs is controlled by the parameters `transform_class_1` and `transform_class_2`, \n",
    "which are transformation matrices applied to the samples of each class. These \n",
    "transformation matrices shape the data distribution by inducing specific covariance \n",
    "matrices of the form $A^T A$. Thus, by applying different transformations to each class, we can\n",
    "generate datasets from Gaussian distributions with different covariance matrices \n",
    "for each class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(n_samples, n_features, transform_class_1, transform_class_2, seed=0):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    X = np.concatenate(\n",
    "        [\n",
    "            rng.randn(n_samples, n_features) @ transform_class_1,\n",
    "            rng.randn(n_samples, n_features) @ transform_class_2 + np.array([1, 1]),\n",
    "        ]\n",
    "    )\n",
    "    y = np.concatenate([np.zeros(n_samples), np.ones(n_samples)])\n",
    "    return X, y\n",
    "\n",
    "\n",
    "transform_matrix = np.array([[1, 0], [0, 1]])\n",
    "X_3, y_3 = make_data(\n",
    "    n_samples=1_000,\n",
    "    n_features=2,\n",
    "    transform_class_1=transform_matrix,\n",
    "    transform_class_2=transform_matrix,\n",
    "    seed=0,\n",
    ")\n",
    "transform_matrix = np.array([[0.0, -0.2], [0.8, 0.2]])\n",
    "X_2, y_2 = make_data(\n",
    "    n_samples=300,\n",
    "    n_features=2,\n",
    "    transform_class_1=transform_matrix,\n",
    "    transform_class_2=transform_matrix,\n",
    "    seed=0,\n",
    ")\n",
    "transform_matrix_1 = np.array([[0.0, -1.0], [2.5, 0.7]]) * 1.5\n",
    "transform_matrix_2 = transform_matrix_1.T\n",
    "X_1, y_1 = make_data(\n",
    "    n_samples=300,\n",
    "    n_features=2,\n",
    "    transform_class_1=transform_matrix_1,\n",
    "    transform_class_2=transform_matrix_2,\n",
    "    seed=0,\n",
    ")\n",
    "\n",
    "# Plot the datasets\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(X_1[:, 0], X_1[:, 1], c=y_1, cmap='coolwarm', edgecolors='k', alpha=0.6)\n",
    "plt.title('Scenario 1')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(X_2[:, 0], X_2[:, 1], c=y_2, cmap='coolwarm', edgecolors='k', alpha=0.6)\n",
    "plt.title('Scenario 2')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(X_3[:, 0], X_3[:, 1], c=y_3, cmap='coolwarm', edgecolors='k',alpha=0.6)\n",
    "plt.title('Scenario 3')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5:** Match each scenario with the corresponding case below. **(1 Point)**\n",
    "\n",
    "- shared, non-isotropic covariance: **TODO**\n",
    "- different covariance: **TODO**\n",
    "- shared, isotropic covariance: **TODO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remember LDA and QDA:\n",
    "\n",
    "- **Multivariate LDA**:\n",
    "$$\n",
    "P(y = k \\mid \\mathbf{x}) \\propto \\exp\\left( -\\frac{1}{2} (\\mathbf{x} - \\mu_k)^\\top \\Sigma^{-1} (\\mathbf{x} - \\mu_k) \\right)\n",
    "$$\n",
    "\n",
    "- **Multivariate LDA Decision (Discriminant) Function**:\n",
    "  $$\n",
    "  \\delta_k(\\mathbf{x}) = \\mathbf{x}^\\top \\Sigma^{-1} \\mu_k - \\frac{1}{2} \\mu_k^\\top \\Sigma^{-1} \\mu_k + \\log \\left(P(y = k)\\right)\n",
    "  $$\n",
    "  - $ \\mu_k $ is the mean vector for class $ k $,\n",
    "  - $ \\Sigma $ is the shared covariance matrix,\n",
    "  - $ |\\Sigma| $ is the determinant of $\\Sigma$,\n",
    "  - $ P(y = k) $ is the prior probability for class $k$.\n",
    "  - Assumes $\\Sigma_1 = \\Sigma_2 = \\dots = \\Sigma_k$. \n",
    "<br /><br /><br /><br />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- **Multivariate QDA**:\n",
    "$$\n",
    "P(y = k \\mid \\mathbf{x}) \\propto \\frac{1}{|\\Sigma_k|^{1/2}} \\exp\\left( -\\frac{1}{2} (\\mathbf{x} - \\mu_k)^\\top \\Sigma_k^{-1} (\\mathbf{x} - \\mu_k) \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "- **QDA Decision (Discriminant) Function**:\n",
    "  $$\n",
    "  \\delta_k(\\mathbf{x}) = -\\frac{1}{2} \\log |\\Sigma_k| - \\frac{1}{2} (\\mathbf{x} - \\mu_k)^\\top \\Sigma_k^{-1} (\\mathbf{x} - \\mu_k) + \\log\\left(P(y = k)\\right)\n",
    "  $$\n",
    "  - $ \\mu_k $ is the mean vector for class $ k $,\n",
    "  - $ \\Sigma_k $ is the covariance matrix for class $ k $,\n",
    "  - $ |\\Sigma_k| $ is the determinant of $ \\Sigma_k $,\n",
    "  - $ P(y = k) $ is the prior probability for class $ k $.\n",
    "  - Assumes $ \\Sigma_1, \\Sigma_2, \\dots, \\Sigma_k $ can be different.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 6:** Assume that we need to perform binary classification for the plotted Scenarios 1, 2, and 3. Choose one classification method (QDA or LDA) to use in each scenario. If you believe both methods are suitable, always select the simpler one. **(1.5 Points)**\n",
    "\n",
    "- Scenario 1: **TODO**\n",
    "- Scenario 2: **TODO**\n",
    "- Scenario 3: **TODO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the performance of LDA and QDA for each case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "from sklearn.discriminant_analysis import (\n",
    "    LinearDiscriminantAnalysis,\n",
    "    QuadraticDiscriminantAnalysis,\n",
    ")\n",
    "\n",
    "fig, axs = plt.subplots(nrows=3, ncols=2, sharex=\"row\", sharey=\"row\", figsize=(16,20))\n",
    "lda = LinearDiscriminantAnalysis(solver=\"svd\", store_covariance=True)\n",
    "qda = QuadraticDiscriminantAnalysis(store_covariance=True)\n",
    "\n",
    "for ax_row, X, y in zip(\n",
    "    axs,\n",
    "    (X_1, X_2, X_3),\n",
    "    (y_1, y_2, y_3),\n",
    "):\n",
    "    lda.fit(X, y)\n",
    "    plot_result(lda, X, y, ax_row[0])\n",
    "    qda.fit(X, y)\n",
    "    plot_result(qda, X, y, ax_row[1])\n",
    "\n",
    "axs[0, 0].set_ylabel(\"Scenario 1\", fontsize=15)\n",
    "axs[1, 0].set_ylabel(\"Scenario 2\", fontsize=15)\n",
    "axs[2, 0].set_ylabel(\"Scenario 3\", fontsize=15)\n",
    "fig.suptitle(\n",
    "    \"Linear Discriminant Analysis (left) vs. Quadratic Discriminant Analysis (right)\",\n",
    "    y=0.94,\n",
    "    fontsize=15,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 7:** As you can see in the scenarios plotted above, LDA (left column) and QDA (right column) do not always result in drastically different decision boundaries. In general, when do they perform similarly, and why? **(1.5 Points)**\n",
    "\n",
    "Your answer: **TODO**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
